{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "614320b0",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2362499892.py, line 189)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [1]\u001b[1;36m\u001b[0m\n\u001b[1;33m    point of this exercise). Use He initialization and the ELU activation function.\u001b[0m\n\u001b[1;37m          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#1. Is it OK to initialize all the weights to the same value as long as that value is selected randomly using He initialization?\n",
    "\n",
    "\"\"\"Initializing all the weights to the same value, even if that value is selected randomly using He initialization, is not \n",
    "   recommended. He initialization is designed to address the vanishing/exploding gradient problem by scaling the initial\n",
    "   weights based on the size of the previous layer. It initializes the weights with random values drawn from a Gaussian\n",
    "   distribution with zero mean and a variance of 2/n, where n is the number of units in the previous layer.\n",
    "\n",
    "   When using He initialization, it is important to introduce randomness in the initial weights to break the symmetry and\n",
    "   allow the network to learn different representations. Initializing all the weights to the same value would result in all\n",
    "   neurons in a given layer computing the same output, making it difficult for the network to learn complex representations \n",
    "   and limiting its capacity.\n",
    "\n",
    "   Therefore, it is generally recommended to initialize weights with random values drawn from a suitable distribution, such\n",
    "   as a Gaussian distribution with zero mean and an appropriate variance, like in He initialization. This helps promote \n",
    "   diversity among the neurons in the network and allows for better learning.\"\"\"\n",
    "\n",
    "#2. Is it OK to initialize the bias terms to 0?\n",
    "\n",
    "\"\"\"Initializing bias terms to 0 is a common and generally acceptable practice in many cases. Bias terms provide the model \n",
    "   with an additional parameter to control the output of a neuron independently of its inputs. They help shift the activation \n",
    "   function, allowing the model to learn and represent more complex functions.\n",
    "\n",
    "   Setting the bias terms to 0 initially can be reasonable, especially when the activation functions used in the network are \n",
    "   symmetric around zero, such as the tanh or ReLU activation functions. In these cases, the initial bias values do not \n",
    "   introduce any bias towards positive or negative values, and the network can learn to adjust the biases during the\n",
    "   training process.\n",
    "\n",
    "   However, in some cases, it might be beneficial to initialize the bias terms with non-zero values, especially when dealing \n",
    "   with activation functions that are not symmetric around zero or when dealing with imbalanced datasets. Initializing biases\n",
    "   with non-zero values can help the network start with an initial bias towards certain outputs, which can accelerate the \n",
    "   learning process and improve performance, especially in the early stages of training.\n",
    "\n",
    "   In summary, initializing bias terms to 0 is generally acceptable and often used as a starting point. However, depending \n",
    "   on the specific problem and network architecture, initializing biases with non-zero values may lead to better results in\n",
    "   certain scenarios. It's always a good practice to experiment and fine-tune the initialization strategy based on the specific\n",
    "   requirements of your model.\"\"\"\n",
    "\n",
    "#3. Name three advantages of the SELU activation function over ReLU.\n",
    "\n",
    "\"\"\"The Scaled Exponential Linear Unit (SELU) activation function offers several advantages over the Rectified Linear Unit\n",
    "   (ReLU) activation function. Here are three advantages of SELU over ReLU:\n",
    "\n",
    "   1. Self-normalization: The SELU activation function is designed to preserve mean and variance of inputs across layers, \n",
    "      leading to self-normalization of the network. It helps mitigate the vanishing/exploding gradient problem and allows \n",
    "      deeper networks to converge more easily. In contrast, ReLU does not have this self-normalization property, and training\n",
    "      deep networks with ReLU can suffer from gradient-related challenges.\n",
    "      \n",
    "   2. Avoiding dead neurons: ReLU neurons can sometimes become \"dead\" and remain inactive for a particular input, causing them\n",
    "      to never activate again during training. Dead neurons do not contribute to the learning process, which can hinder the\n",
    "      capacity of the network. SELU helps mitigate this issue by preventing the occurrence of dead neurons, resulting in more\n",
    "      active and effective neurons throughout the network.   \n",
    "      \n",
    "   3. Improved performance: SELU has been shown to provide improved performance in certain types of neural networks, \n",
    "      particularly in deep architectures. It has demonstrated better accuracy and convergence rates on various tasks,\n",
    "      such as image classification, compared to ReLU-based networks. SELU achieves this by leveraging its self-normalization \n",
    "      and preventing dead neurons, allowing for more efficient and effective learning.\n",
    "\n",
    "  It's worth noting that the advantages of SELU over ReLU may not be universal and can depend on the specific problem, network \n",
    "  architecture, and data characteristics. However, SELU is a promising alternative to ReLU, especially when working with deep\n",
    "  neural networks, and it has gained attention for its ability to address some of the limitations associated with ReLU.\"\"\"\n",
    "\n",
    "#4. In which cases would you want to use each of the following activation functions: SELU, leaky ReLU (and its variants), \n",
    "ReLU, tanh, logistic, and softmax?\n",
    "\n",
    "\"\"\"Different activation functions have their strengths and are suitable for specific scenarios. Here's a breakdown of when you\n",
    "   might want to use each of the mentioned activation functions:\n",
    "   \n",
    "   1. SELU (Scaled Exponential Linear Unit):\n",
    "      Deep neural networks: SELU can be beneficial when training deep neural networks due to its self-normalizing property,\n",
    "      which helps mitigate the vanishing/exploding gradient problem. It is particularly useful in architectures where layer \n",
    "      normalization or other normalization techniques are not applied.\n",
    "      \n",
    "   2. Leaky ReLU (and its variants):\n",
    "      Sparse activation: Leaky ReLU allows a small positive slope for negative inputs, which helps prevent dead neurons and \n",
    "      encourages the propagation of gradients for inputs below zero. It can be useful when you want to introduce some level \n",
    "      of sparsity in the network and avoid complete suppression of negative values.\n",
    "      \n",
    "   3. ReLU (Rectified Linear Unit):\n",
    "      Simplicity and efficiency: ReLU is widely used in various deep learning architectures due to its simplicity and \n",
    "      computational efficiency. It helps with non-linearity and has proven effective in many cases, especially when the\n",
    "      data and problem domain are well-suited for ReLU activation (e.g., image classification).   \n",
    "      \n",
    "   4. tanh (Hyperbolic Tangent):\n",
    "      Symmetric activation: tanh is often used in scenarios where activation functions with outputs ranging from -1 to 1 are \n",
    "      desired. It can be useful when the data is centered around zero or when you want to preserve negative and positive values\n",
    "      in the output. \n",
    "      \n",
    "   5. logistic (Sigmoid):\n",
    "      Binary classification: The logistic activation function is commonly used in binary classification tasks where the output \n",
    "      is expected to represent probabilities between 0 and 1. It is well-suited for logistic regression models and can be used \n",
    "      as the final layer activation function in a binary classification neural network. \n",
    "      \n",
    "   6. softmax:\n",
    "      Multiclass classification: Softmax activation is commonly used in the output layer of neural networks for multiclass \n",
    "      classification problems. It normalizes the outputs to represent probabilities, ensuring that the sum of the outputs\n",
    "      across classes is equal to 1. Softmax is suitable when you need to make mutually exclusive class predictions.\n",
    "      \n",
    " It's important to note that the choice of activation function can also depend on experimentation and the specific\n",
    " characteristics of the problem, as there is no one-size-fits-all solution. Different activation functions might yield\n",
    " different results, and it is often beneficial to try out multiple options to determine the most effective choice for a\n",
    " given task.\"\"\"\n",
    "\n",
    "#5. What may happen if you set the momentum hyperparameter too close to 1 (e.g., 0.99999) when using an SGD optimizer?\n",
    "\n",
    "\"\"\"When setting the momentum hyperparameter too close to 1 (e.g., 0.99999) in an SGD optimizer, several potential issues can \n",
    "   arise:\n",
    "\n",
    "   1. Slow convergence: A high momentum value means that the influence of previous gradients on the current update is\n",
    "      significantly amplified. When momentum is close to 1, the accumulated momentum from previous updates can dominate\n",
    "      the current gradient information, leading to slow convergence. This can cause the optimization process to take longer\n",
    "      to reach the optimal solution or even get stuck in suboptimal regions.\n",
    "\n",
    "   2. Overshooting the minimum: Momentum helps accelerate gradient descent by carrying along a portion of the previous update \n",
    "      direction. However, with a momentum value close to 1, the updates can become very large, leading to overshooting the\n",
    "      minimum. The momentum's persistent effect can cause the optimizer to overshoot the optimal point repeatedly, resulting \n",
    "      in oscillation and instability during training.\n",
    "      \n",
    "   3. Difficulty escaping local minima: Setting the momentum too close to 1 can make it harder for the optimizer to escape \n",
    "      local minima. The large accumulated momentum prevents the optimizer from effectively exploring other areas of the \n",
    "      parameter space, potentially trapping the optimization process in a suboptimal solution. \n",
    "      \n",
    "   4. Sensitivity to learning rate: Momentum is closely related to the learning rate in the optimization process. A high\n",
    "      momentum value amplifies the effect of the learning rate, making the optimization process more sensitive to its value. \n",
    "      If the learning rate is not properly tuned, a momentum value close to 1 can lead to erratic behavior and make the\n",
    "      optimization process highly unstable.\n",
    "\n",
    " In summary, setting the momentum hyperparameter too close to 1 in an SGD optimizer can result in slow convergence,\n",
    " overshooting of the minimum, difficulty escaping local minima, and increased sensitivity to the learning rate. It is\n",
    " generally recommended to choose a momentum value that strikes a balance between the benefits of momentum (accelerated\n",
    " convergence, escaping shallow minima) and the risk of overshooting and instability. Experimentation and fine-tuning are\n",
    " crucial to finding an appropriate momentum value for a specific optimization task.\"\"\"\n",
    "\n",
    "#6. Name three ways you can produce a sparse model.\n",
    "\n",
    "\"\"\"To produce a sparse model, where a significant number of weights or parameters are set to zero, you can employ the \n",
    "   following techniques:\n",
    "\n",
    "   1. L1 Regularization (Lasso regularization):\n",
    "      Applying L1 regularization during the training process encourages sparsity in the model by adding a penalty term based \n",
    "      on the absolute values of the weights. This penalty term pushes the optimizer to minimize the number of non-zero weights, \n",
    "      effectively promoting sparsity. L1 regularization can be added to the loss function as a regularization term, and the \n",
    "      regularization strength can be controlled using a hyperparameter.\n",
    "      \n",
    "   2. Group Lasso or Group Sparsity:\n",
    "      Group Lasso is an extension of L1 regularization that promotes sparsity at the group level. Instead of penalizing \n",
    "      individual weights, Group Lasso encourages groups of related weights to be set to zero. This approach is useful when \n",
    "      there are groups of weights that are expected to be either all active or all zero. Group Lasso is particularly beneficial \n",
    "      in scenarios such as feature selection or convolutional neural networks, where weights in each filter can be grouped.  \n",
    "      \n",
    "   3. Pruning:\n",
    "      Pruning involves iteratively removing or zeroing out the less important weights in a pre-trained model. It can be \n",
    "      performed using various criteria, such as magnitude-based pruning, where weights below a certain threshold are pruned. \n",
    "      Magnitude-based pruning removes weights that contribute less to the overall network output, thus inducing sparsity.\n",
    "      Another approach is structured pruning, where entire filters, layers, or groups of weights are pruned based on specific \n",
    "      criteria.\n",
    "\n",
    " These techniques can be used individually or in combination to achieve sparsity in a model. By encouraging sparsity, these\n",
    " methods help reduce the memory footprint of the model, improve computational efficiency, and potentially enhance \n",
    " interpretability by focusing on the most important weights or features. It's important to note that sparse models may\n",
    " require specialized techniques for efficient implementation and deployment, depending on the target hardware or framework.\"\"\"\n",
    "\n",
    "#7. Does dropout slow down training? Does it slow down inference (i.e., making predictions on new instances)? What about MC\n",
    "Dropout?\n",
    "\n",
    "\"\"\"Dropout can introduce a slight slowdown during training due to its stochastic nature. During each training iteration, \n",
    "   dropout randomly sets a fraction of the neurons' outputs to zero, which effectively creates an ensemble of different \n",
    "   network architectures. This randomness requires recalculating the activations for each training instance, resulting in\n",
    "   increased computation time compared to a network without dropout.\n",
    "\n",
    "   However, dropout itself does not significantly impact inference time or the process of making predictions on new instances.\n",
    "   During inference, dropout is typically turned off, and the full network is used to make predictions without any \n",
    "   stochasticity. Therefore, the inference time remains comparable to the time it would take to make predictions with a\n",
    "   network without dropout.\n",
    "\n",
    "   MC Dropout (Monte Carlo Dropout) is an extension of dropout that can be used for uncertainty estimation in deep learning \n",
    "   models. It involves performing multiple forward passes with dropout enabled during inference and averaging the predictions\n",
    "   to estimate uncertainty. Each forward pass with dropout generates different predictions due to the stochastic nature of\n",
    "   dropout. While this process introduces some additional computation time compared to a deterministic prediction, it is\n",
    "   still feasible for most applications.\n",
    "\n",
    "   In summary, dropout can slightly slow down training due to the additional computations required for each training iteration.\n",
    "   However, dropout does not significantly affect inference time or the process of making predictions on new instances.\n",
    "   MC Dropout, on the other hand, introduces a minor additional computational overhead during inference to estimate \n",
    "   uncertainty, but it is still practical for most applications.\"\"\"\n",
    "\n",
    "#8. Practice training a deep neural network on the CIFAR10 image dataset:\n",
    "\n",
    "#a. Build a DNN with 20 hidden layers of 100 neurons each (that’s too many, but it’s the\n",
    "point of this exercise). Use He initialization and the ELU activation function.\n",
    "\n",
    "\"\"\"Certainly! Here's an example of building a deep neural network with 20 hidden layers of 100 neurons each using He \n",
    "   initialization and the ELU activation function on the CIFAR-10 dataset.\n",
    "   \n",
    "   import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Load the CIFAR-10 dataset\n",
    "(X_train, y_train), (X_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "# Preprocess the data\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "# Build the deep neural network\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=(32, 32, 3)))\n",
    "\n",
    "# Add 20 hidden layers with 100 neurons each\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"))\n",
    "\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "  In this example, we use the Sequential API from TensorFlow to build the model. The Flatten layer is added as the input layer \n",
    "  to flatten the input images. Then, we add 20 hidden layers with 100 neurons each, using the ELU activation function and He \n",
    "  initialization (kernel_initializer=\"he_normal\"). Finally, a dense output layer with 10 neurons (corresponding to the 10 \n",
    "  classes in CIFAR-10) and softmax activation is added.\n",
    "\n",
    "  The model is compiled with the sparse_categorical_crossentropy loss function for multi-class classification and the adam \n",
    "  optimizer. We then train the model on the CIFAR-10 training data for 10 epochs, using a batch size of 32. The validation \n",
    "  data is used to evaluate the model's performance during training.\n",
    "\n",
    "  Please note that training a deep neural network with 20 hidden layers and 100 neurons each can be computationally expensive \n",
    "  and may require a significant amount of time and resources. It is recommended to consider using a smaller network or \n",
    "  leveraging techniques like regularization or early stopping to avoid overfitting.\"\"\"\n",
    "\n",
    "#b. Using Nadam optimization and early stopping, train the network on the CIFAR10\n",
    "dataset. You can load it with keras.datasets.cifar10.load_​data(). The dataset is\n",
    "composed of 60,000 32 × 32–pixel color images (50,000 for training, 10,000 for\n",
    "testing) with 10 classes, so you’ll need a softmax output layer with 10 neurons.\n",
    "Remember to search for the right learning rate each time you change the model’s\n",
    "architecture or hyperparameters.\n",
    "\n",
    "\"\"\"Certainly! Here's an example of training a deep neural network on the CIFAR-10 dataset using Nadam optimization and early \n",
    "   stopping.\n",
    "   \n",
    "   import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Load the CIFAR-10 dataset\n",
    "(X_train, y_train), (X_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "# Preprocess the data\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "# Define the model architecture\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Conv2D(32, (3, 3), activation='elu', padding='same', input_shape=(32, 32, 3)),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Conv2D(32, (3, 3), activation='elu', padding='same'),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.MaxPooling2D((2, 2)),\n",
    "    keras.layers.Dropout(0.25),\n",
    "\n",
    "    keras.layers.Conv2D(64, (3, 3), activation='elu', padding='same'),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Conv2D(64, (3, 3), activation='elu', padding='same'),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.MaxPooling2D((2, 2)),\n",
    "    keras.layers.Dropout(0.25),\n",
    "\n",
    "    keras.layers.Conv2D(128, (3, 3), activation='elu', padding='same'),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Conv2D(128, (3, 3), activation='elu', padding='same'),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.MaxPooling2D((2, 2)),\n",
    "    keras.layers.Dropout(0.25),\n",
    "\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(128, activation='elu'),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = keras.optimizers.Nadam(learning_rate=0.001)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "# Define early stopping\n",
    "early_stopping = keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=64, validation_data=(X_test, y_test), callbacks=[early_stopping])\n",
    "\n",
    "  In this example, we use a deep CNN architecture with convolutional layers, batch normalization, max pooling, and dropout \n",
    "  for regularization. The model is compiled with the Nadam optimizer and sparse categorical cross-entropy loss for multi-class \n",
    "  classification. We define early stopping with a patience of 5 epochs to monitor validation loss and restore the best weights.\n",
    "\n",
    "  During training, the model is trained for a maximum of 50 epochs, with a batch size of 64. The validation data is used to \n",
    "  monitor the model's performance and apply early stopping to prevent overfitting.\n",
    "\n",
    "  To find the right learning rate, you can use techniques such as learning rate schedules, grid search, or automated tools \n",
    "  like the Learning Rate Finder or Cyclical Learning Rates. Experimenting with different learning rates can help find the \n",
    "  optimal value that leads to faster convergence and better performance.\n",
    "\n",
    "  Note that the architecture and hyperparameters provided in this example may not be optimal for your specific use case, so \n",
    "  feel free to modify them and tune them further as needed.\"\"\"\n",
    "\n",
    "#c. Now try adding Batch Normalization and compare the learning curves: Is it converging faster than before? Does it produce \n",
    "a better model? How does it affect training speed?\n",
    "\n",
    "\"\"\"Certainly! Adding Batch Normalization to the deep neural network trained on the CIFAR-10 dataset can have several effects \n",
    "   on the learning curves, convergence speed, model performance, and training speed. Let's examine each aspect:\n",
    "   \n",
    "   1. Convergence speed: With Batch Normalization, the learning curves are likely to show faster convergence compared to the\n",
    "      same network architecture without Batch Normalization. Batch Normalization helps stabilize and accelerate training by\n",
    "      normalizing the inputs to each layer, reducing the internal covariate shift problem. This allows for more stable gradients\n",
    "      and enables the network to converge faster.\n",
    "      \n",
    "   2. Model performance: Batch Normalization can lead to improved model performance. By normalizing the inputs, Batch \n",
    "      Normalization helps address issues like vanishing/exploding gradients, improving the overall stability and generalization\n",
    "      capability of the model. It can also reduce overfitting by acting as a form of regularization. As a result, Batch \n",
    "      Normalization can contribute to better accuracy and lower validation loss, leading to a better model. \n",
    "      \n",
    "   3. Training speed: While Batch Normalization can accelerate convergence, it may slightly slow down the training process \n",
    "      per epoch. This is because Batch Normalization involves additional computations during training to compute and update\n",
    "      the mean and variance of each batch. However, the overall training time can still be faster due to faster convergence\n",
    "      and the ability to use higher learning rates. The exact impact on training speed depends on factors such as the model \n",
    "      architecture, batch size, and hardware.\n",
    "\n",
    " To observe these effects, you can modify the previous example by adding Batch Normalization layers after each Conv2D layer.\n",
    " Here's an updated code snippet:   \n",
    " \n",
    " import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# ... Load and preprocess CIFAR-10 data ...\n",
    "\n",
    "# Define the model architecture with Batch Normalization\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Conv2D(32, (3, 3), activation='elu', padding='same', input_shape=(32, 32, 3)),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Conv2D(32, (3, 3), activation='elu', padding='same'),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.MaxPooling2D((2, 2)),\n",
    "    keras.layers.Dropout(0.25),\n",
    "\n",
    "    # ... Additional Conv2D layers with Batch Normalization ...\n",
    "\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(128, activation='elu'),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# ... Define optimizer, compile model, define early stopping ...\n",
    "\n",
    "# Train the model with Batch Normalization\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=64, validation_data=(X_test, y_test), callbacks=[early_stopping])\n",
    "\n",
    "\n",
    "  By adding Batch Normalization layers after each Conv2D layer, you can observe the effects on the learning curves, \n",
    "  convergence speed, model performance, and training speed. It is recommended to compare the results with the previous \n",
    "  training (without Batch Normalization) to understand the impact of Batch Normalization on your specific deep neural \n",
    "  network architecture and the CIFAR-10 dataset.\"\"\"\n",
    "\n",
    "#d. Try replacing Batch Normalization with SELU, and make the necessary adjustements\n",
    "to ensure the network self-normalizes (i.e., standardize the input features, use\n",
    "LeCun normal initialization, make sure the DNN contains only a sequence of dense\n",
    "layers, etc.).\n",
    "\n",
    "\"\"\"Certainly! To replace Batch Normalization with SELU (Scaled Exponential Linear Units) and ensure that the network\n",
    "   self-normalizes, there are a few adjustments that need to be made. Here's an example of training a deep neural network \n",
    "   on the CIFAR-10 dataset using SELU activation and the necessary adjustments:\n",
    "   \n",
    "   import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import initializers\n",
    "\n",
    "# Load the CIFAR-10 dataset\n",
    "(X_train, y_train), (X_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "# Preprocess the data and standardize the input features\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "# Ensure the network self-normalizes\n",
    "# Use LeCun normal initialization for weights\n",
    "# Use SELU activation for all hidden layers\n",
    "\n",
    "# Build the deep neural network with SELU activation\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=(32, 32, 3)))\n",
    "\n",
    "# Add dense layers with SELU activation\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100, activation=\"selu\", kernel_initializer=initializers.lecun_normal()))\n",
    "\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "\n",
    "  In this example, we standardize the input features by dividing the pixel values by 255.0 to ensure that they are in the\n",
    "  range [0, 1]. This is important for the self-normalization properties of SELU.\n",
    "\n",
    "  To ensure self-normalization with SELU, the following adjustments are made:\n",
    "\n",
    "  . We use LeCun normal initialization (kernel_initializer=initializers.lecun_normal()) for the weights. This initialization\n",
    "    is designed to work well with SELU activation.\n",
    "    \n",
    "  . All hidden layers use the SELU activation function (activation=\"selu\"). It is important to note that the input layer \n",
    "    does not use SELU activation because the input features have been standardized.\n",
    "    \n",
    " The rest of the code remains similar to the previous example. The model is compiled with the Adam optimizer and sparse \n",
    " categorical cross-entropy loss. It is trained on the CIFAR-10 training data for 10 epochs with a batch size of 32, and \n",
    " the validation data is used to evaluate the model's performance during training.\n",
    "\n",
    " By using SELU activation and ensuring self-normalization, the network can achieve stable training and potentially better\n",
    " performance. It is recommended to experiment with different hyperparameters, architectures, and regularization techniques \n",
    " to further optimize the model's performance on the CIFAR-10 dataset.\"\"\"\n",
    "\n",
    "#e. Try regularizing the model with alpha dropout. Then, without retraining your model, see if you can achieve better\n",
    "accuracy using MC Dropout.\n",
    "\n",
    "\"\"\"To regularize the model with alpha dropout and then evaluate the performance using MC Dropout, you can make the following \n",
    "   adjustments:\n",
    "\n",
    "  1. Regularize the model with alpha dropout:\n",
    "  \n",
    "   import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Load the CIFAR-10 dataset\n",
    "(X_train, y_train), (X_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "# Preprocess the data\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "# Build the deep neural network\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=(32, 32, 3)))\n",
    "\n",
    "# Add hidden layers with alpha dropout\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100, activation=\"selu\"))\n",
    "    model.add(keras.layers.AlphaDropout(0.25))\n",
    "\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "  In this code, alpha dropout is used as a form of regularization. Alpha dropout differs from traditional dropout in that it\n",
    "  randomly sets inputs to a negative value instead of zero, helping to preserve the mean and variance of the inputs. The\n",
    "  AlphaDropout layer is added after each dense layer with a dropout rate of 0.25.\n",
    "\n",
    "  2. Evaluate the model using MC Dropout:\n",
    "  \n",
    "  # Predict using MC Dropout\n",
    "y_pred = np.mean([model.predict(X_test, training=True) for _ in range(100)], axis=0)\n",
    "accuracy = np.mean(np.argmax(y_pred, axis=1) == y_test.flatten())\n",
    "print(\"MC Dropout Accuracy:\", accuracy)\n",
    "\n",
    "  After training the model with alpha dropout, you can use MC Dropout to evaluate the performance without retraining the model.\n",
    "  MC Dropout involves performing multiple predictions with dropout active during inference and then averaging the predictions. \n",
    "  In this code snippet, 100 predictions are made using model.predict(X_test, training=True), which activates dropout during\n",
    "  inference. The predictions are then averaged, and the accuracy is calculated.\n",
    "\n",
    "  By incorporating alpha dropout for regularization during training and utilizing MC Dropout during evaluation, you can assess \n",
    "  whether MC Dropout improves the accuracy of the model without retraining. It is important to note that MC Dropout can \n",
    "  introduce additional computational overhead during inference due to multiple predictions, but it can help provide more \n",
    "  robust uncertainty estimation and potentially improve accuracy in certain scenarios.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074a6600",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
